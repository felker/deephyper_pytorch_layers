# deephyper_pytorch_layers
Use DeepHyper to find the max FLOPS for various PyTorch Layers.



## TODO:

- [ ] Benchmark DeepHyper+Balsam throughput on various platforms.
Taylor reports low utilization on Theta.



### More PyTorch layers, etc.
- [ ] `BatchNorm1d`
- [ ] `BatchNorm2d`
- [ ] `BatchNorm3d`
- [ ] `Softmax`
- [ ] `Tanh`
- [ ] `Sigmoid`
- [ ] `ReLU`
- [ ] Pooling?
- [ ] Embedding?
- [ ] `RNN`
- [ ] `LSTM`
- [ ] `GRU`
- [ ] `Transformer`, `TransformerEncoder`, `TransformerDecoder`

See https://pytorch.org/docs/stable/nn.html
